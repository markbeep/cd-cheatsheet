\section*{Optimizations}
 
There are different kinds of optimization: Power, Space, Time.

\begin{compactitem}
	\item \textbf{Constant Folding}: If operands are statically known, compute value at compile-time. More general algebraic simplification: Use mathematical identities.
	
	\item \textbf{Constant Propagation}: If $x$ is a constant replace its uses by the constant.
	
	\item \textbf{Copy Propagation}: For $x = y$ replace uses of $x$ with $y$
	
	\item \textbf{Dead Code Elimination}: If side-effect free code can never be observed, safe to eliminate it.
	
	\item \textbf{Inlining}: Replace a function call with the body of the function (arguments are rewritten to local variables).
	
	\item \textbf{Code Specialization}: Create Specialized versions of a function that is called form different places with different arguments.
	
	\item \textbf{Common Subexpression Elimination}: It is the opposite of inlining, fold redundant computations together.
	
	\item \textbf{Loop Optimizations}
	\begin{compactitem}
		\item Hot spots often occur in loops (esp. inner loops)
		\item Loop Invariant Code Motion (hoist outside)
		\item Strength Reduction (replace expensive ops by cheap ones by creating a dependent induction variable)
		\item Loop Unrolling
	\end{compactitem}
\end{compactitem}


\subsection*{Dataflow Analysis}

Almost every dataflow analysis is a variation of the following algorithm. \smallskip

\textbf{Forward Must Dataflow Analysis}
\begin{lstlisting}
	for all n, in[n] = T, out[n] = T
	repeat until no change in 'in' or 'out'
		for all n
			in[n] = intersect out[n`] for all n` in pred[n]
			out[n] = gen[n] union (in[n] \ kill[n])	
\end{lstlisting}\medskip

\textbf{Backward}: swap \texttt{in} and \texttt{out} and \texttt{pred} with \texttt{succ}.\medskip

\textbf{May}: swap $\top$ with $\bot$ or $\emptyset$ and replace $\cap$ with $\cup$.\medskip

For each dataflow analysis we only need to define the set \texttt{gen}, \texttt{kill} as well as the domain of dataflow values $\mathcal L$ and a combining operator $\cup$ or $\cap$.\medskip

\textbf{Liveness} (Backward, May)\medskip

We can use the same registers for multiple \texttt{\%uids} if they are not alive at the same time. We define \texttt{gen[s]} as all the variables used and \texttt{kill[s]} as all the variables defined by statement $s$. $\mathcal L$ corresponds to the variables and the combination operator to the set union.\medskip

It holds: in[$n$] $\supseteq$ gen[$n$], in[$n$] $\supseteq$ out[$n$] $\backslash$ kill[$n$] and out[$n$] $\supseteq$ in[$n'$] if $n' \in $ succ[$n$].\medskip

\textbf{Reaching Definition} (Forward, May)\medskip

What variable definitions reach a particular use of a variable? Used for constant and copy propagation. \texttt{in / out} is the set of nodes defining some variable such that the definition may reach the beginning resp. end of the current node. \texttt{gen} is the current statement, if it defines a variable, and \texttt{kill} is all the other definitions of the defined variable. \medskip

\textbf{Available Expressions} (Forward, Must)\medskip

Used for common subexpression elimination. \texttt{in / out} are the set of nodes whose values are available on entry / exit of the current node. \texttt{gen} is the current statement and \texttt{kill} is the set of all expressions that use the newly modified variable.\medskip

\textbf{Very Busy} (Backward, Must)\medskip

An expression is very busy at location $p$, if every path from $p$ must evaluate the expression before any variable is redefined. It is used for hoisting expressions.\smallskip

\texttt{gen[B]} = \{expr; expr a op b is evaluated in B, neither a nor b are subsequently redefinded in B \}\smallskip

\texttt{kill[B]} = \{expr; a or b of expr a op b are defined in B and a op b is not subsequently evaluated in B\} \medskip

\textbf{Dominators} (Forward, Must)\medskip

Define \texttt{dom[n]} as the set of all nodes that dominate \texttt{n}, i.e. \texttt{dom[n] = out[n]}, \texttt{gen} is the singelton set of the node itself, \texttt{kill} is the empty set.\medskip

The iterative solution computes the ideal meet-over-path solution if the flow function distributes over $\cap$. Most of the problems that express properties on how the program computes are distributive and compute the MOP solution, analyses of what the program computes do not (e.g. constant propagation). Our analyses also always terminate, as the flow function (\texttt{out[n] = ...}) is monotonic.\medskip

\textbf{Soundness} is defined as an under approximation of the set of variables.


\subsection*{Register Allocation}

\textbf{Linear-Scan Register Allocation}\medskip

Compute liveness information and then scan through the program, for each instruction try to find an available register, else spill it on the stack.\medskip
	
\textbf{Graph Coloring}\medskip

Compute liveness information for each temp, create an inference graph (nodes are temps and there is an edge if they are alive at the same time), try to color the graph.\medskip
	
\textbf{Kempe's Algorithm}: 
\begin{compactitem}
	\item Find a node with degree $< k$ and cut it out of the graph
	\item Recursively $k$-color the remaining subgraph
	\item When remaining graph is colored, there must be at least one free color available for the deleted node.
	\item If the graph cannot be colored we spill a node and try again.
\end{compactitem}\medskip

This can be improve by adding \texttt{move} related edges (temps used in a move should have the same color). More aggresively, we may coalesce two move-related nodes into one. This may increase the degree of a node, so we need to be careful. \medskip

\textbf{Brigg}'s strategy is to only coalesce if the resulting node has fewer than $k$ neighbors with degree $\geq k$. \medskip

\textbf{George}'s strategs is to only coalesce if for every neighbor $t$ of one of the coalescing nodes $x$, $t$ also interferes with the other coalescing node or $t$ has degree $< k$.\medskip
	
Precolored Nodes: Certain variables must be pre-assigned to registers (\texttt{call}, \texttt{imul}, caller-save registers)



\subsection*{Dominator Trees}

We want to identify loops in a CFG. For that we use domination. $A$ dominates $B$ ($A$ dom $B$), if the only way to reach $B$ from start node is via $A$. This relation is transitive, reflexive and anti-symmetric. This can be computed as forward must dataflow analysis. $A$ strictly dominate $B$, if $A \neq B$ and $A$ dom $B$.\medskip

The Hasse diagram of the dominates relation is called the dominator tree.\medskip

A loop is a set of nodes in the CFG, with a distinguished entry, the header, and exit nodes. It is a strongly connected component (SSC), every node is reachable from every other node and vice versa.\medskip

A loop contains at least 1 back edge (back edge = target dominates the source).\medskip

The \textbf{dominance frontier} of a node $A$ is the set of all CFG nodes $\gamma$ such that $A$ dominates a predecessor of $\gamma$, but does not strictly dominate $\gamma$. Intuitively: starting at $A$, there is a path to $\gamma$, but there is another route that does not go through $A$. It is the set of nodes where $A$'s dominance stops.


\subsection*{Single Static Assignment (SSA)}

Each LLVM IR \texttt{\%uid} can be assigned only once. When coming from an \texttt{if-else} branch or similar, we might not know which \texttt{\%uid} to take. That's where we introduce $\phi$-nodes.\medskip

A $\phi$-node picks the version of a variable depending on the label from which the $\phi$-node was entered. It even allows usage of later-defined \texttt{\%uid}s.\medskip

\begin{lstlisting}
	%uid = phi <type> v1, <label1>, ..., vn, <labeln>
\end{lstlisting}\medskip

Converting to SSA:
\begin{compactitem}
	\item Start with CFG with \texttt{alloca}s, identify promotable \texttt{alloca}s
	\item Compute dominator tree information
	\item Calculate \texttt{def} / \texttt{use} information for each variable
	\item Insert $\phi$-nodes at necessary join points
	\item Replace \texttt{load} / \texttt{stores} with freshly generated \texttt{\%uid}s
	\item Eliminate unneeded \texttt{alloca}s
\end{compactitem}\medskip

Some \texttt{alloca}s are needed, either if the address of the variable is taken or the address escapes by being passed to a function. If neither condition holds, it is promotable.\medskip

Necessary join points are defined as the transitive closure of the dominance frontier of all nodes where a variable $x$ is defined or modified. Then we just need to pick the value of $x$ depending on the predecessors of the node where we just inserted the $\phi$-node.\medskip

To place $\phi$-nodes without breaking SSA, we insert \texttt{load}s at the end of each block, and insert \texttt{store}s after $\phi$-nodes. We can then optimize load after stores (LAS) by substituting all uses of the load by the value stored and remove the load itself. Then, we can eliminate dead \texttt{stores} and dead \texttt{alloca}s. At the very end, we can eliminate $\phi$-nodes with only a single value, or identical values from each predecessor.
